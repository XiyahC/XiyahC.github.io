<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>NLP-Connecting Language to the World | XiyahC</title><meta name="keywords" content="NLP"><meta name="author" content="Xinyu(Xiyah) Chang"><meta name="copyright" content="Xinyu(Xiyah) Chang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="connect vision - language generative vision-language model others[speech, audio] from language to code from language to action  1.connect vision - languageHistory1960s first cv project.2000s shallow">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP-Connecting Language to the World">
<meta property="og:url" content="https://xiyahc.github.io/2024/05/23/NLP-Connect-LMs-to-the-world/index.html">
<meta property="og:site_name" content="XiyahC">
<meta property="og:description" content="connect vision - language generative vision-language model others[speech, audio] from language to code from language to action  1.connect vision - languageHistory1960s first cv project.2000s shallow">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://images.naturecraft.world/images/2021/06/17/wl-op-24se.jpg">
<meta property="article:published_time" content="2024-05-23T04:00:00.000Z">
<meta property="article:modified_time" content="2024-05-24T22:30:06.471Z">
<meta property="article:author" content="Xinyu(Xiyah) Chang">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://images.naturecraft.world/images/2021/06/17/wl-op-24se.jpg"><link rel="shortcut icon" href="https://images.naturecraft.world/images/2021/06/17/wlop-5se.jpg"><link rel="canonical" href="https://xiyahc.github.io/2024/05/23/NLP-Connect-LMs-to-the-world/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'NLP-Connecting Language to the World',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-05-24 18:30:06'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://images.naturecraft.world/images/2024/02/23/selfie.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">11</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">9</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/portfolio/"><i class="fa-fw fa-solid fa-star"></i><span> Portfolio</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-solid fa-school"></i><span> Education</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/UndergradUW/"><i class="fa-fw fa-solid fa-w"></i><span> Undergrad-UW</span></a></li><li><a class="site-page child" href="/GradJHU/"><i class="fa-fw fa-solid fa-j"></i><span> Grad-JHU</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-solid fa-person-skating"></i><span> OutOfSchool</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/Bake/"><i class="fa-fw fa-solid fa-cookie-bite"></i><span> Baking</span></a></li><li><a class="site-page child" href="/FrontEnd/"><i class="fa-fw fa-solid fa-laptop"></i><span> FrontEnd</span></a></li><li><a class="site-page child" href="/DS/"><i class="fa-fw fa-solid fa-database"></i><span> DataScience</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Resume</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://images.naturecraft.world/images/2021/06/17/wl-op-24se.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">XiyahC</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/portfolio/"><i class="fa-fw fa-solid fa-star"></i><span> Portfolio</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-solid fa-school"></i><span> Education</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/UndergradUW/"><i class="fa-fw fa-solid fa-w"></i><span> Undergrad-UW</span></a></li><li><a class="site-page child" href="/GradJHU/"><i class="fa-fw fa-solid fa-j"></i><span> Grad-JHU</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-solid fa-person-skating"></i><span> OutOfSchool</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/Bake/"><i class="fa-fw fa-solid fa-cookie-bite"></i><span> Baking</span></a></li><li><a class="site-page child" href="/FrontEnd/"><i class="fa-fw fa-solid fa-laptop"></i><span> FrontEnd</span></a></li><li><a class="site-page child" href="/DS/"><i class="fa-fw fa-solid fa-database"></i><span> DataScience</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Resume</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">NLP-Connecting Language to the World</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-05-23T04:00:00.000Z" title="Created 2024-05-23 00:00:00">2024-05-23</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-05-24T22:30:06.471Z" title="Updated 2024-05-24 18:30:06">2024-05-24</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="NLP-Connecting Language to the World"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><ol>
<li>connect vision - language</li>
<li>generative vision-language model</li>
<li>others[speech, audio]</li>
<li>from language to code</li>
<li>from language to action</li>
</ol>
<h1 id="1-connect-vision-language"><a href="#1-connect-vision-language" class="headerlink" title="1.connect vision - language"></a>1.connect vision - language</h1><h2 id="History"><a href="#History" class="headerlink" title="History"></a>History</h2><p>1960s first cv project.<br>2000s shallow classifiers and feature engineering.<br>2012  deep learning revolution.<br>    CNN in ImageNet.<br>    unification of architectures.<br>    Rise of image generation (VAEs, GANs, etc.).<br>2020s eras of vision transformer.  </p>
<h2 id="How-to-encode-images"><a href="#How-to-encode-images" class="headerlink" title="How to encode images?"></a>How to encode images?</h2><p><strong>Vision Transformers (ViT)</strong>.  Image to patch(matrices, e.g. you have different channels for different colors) + position embedding. Feed these channels into transformers.  </p>
<h2 id="How-to-encode-paired-image-text"><a href="#How-to-encode-paired-image-text" class="headerlink" title="How to encode paired image-text?"></a>How to encode paired image-text?</h2><p>Idea: create a space to represent both semantics of language and image. (This is similar to the idea while I use Meta’s laser_encoders to complete multilingual tasks).  </p>
<p>Then create a model that can align semantically-equivalent text and images nearby.  </p>
<p>把image和text转到同一个坐标系里，相关度高的图片和文字如猫猫图片+“A cat”，在坐标系里就会离得近。  </p>
<h2 id="Contrastive-Language-Image-Pre-training-CLIP"><a href="#Contrastive-Language-Image-Pre-training-CLIP" class="headerlink" title="Contrastive Language-Image Pre-training (CLIP)"></a>Contrastive Language-Image Pre-training (CLIP)</h2><p>Having two encoders: text encoder, image encoder. Having a function $F$, for every text and image pair, if they are align, then $F=1$, if they are not align, $F$ is close to zero.</p>
<p>In CLIP, the description is for the whole image, not partial image. </p>
<p>Tasks CLIP can do: classification. it gives probabilities of things.<br>Demo: gives an image of JHU campus, gives several keywords: Johns Hopkins, Stanford, Berkeley, UPenn. CLIP assign Johns Hopkins with highest probabilities.  </p>
<p>No generation capabilities; Prompting / In-Context Learning: Few-shot captioning.  </p>
<h2 id="What-happened-after-CLIP"><a href="#What-happened-after-CLIP" class="headerlink" title="What happened after CLIP?"></a>What happened after CLIP?</h2><ol>
<li>Data scaling up</li>
<li>Model design</li>
<li>Objective functions(e.g. Contrastive learning)</li>
</ol>
<p>Open source model: OpenCLIP.<br>Pre-training on LAION-5B dataset.  </p>
<h1 id="2-generative-vision-language-model"><a href="#2-generative-vision-language-model" class="headerlink" title="2.generative vision-language model"></a>2.generative vision-language model</h1><h2 id="Image-Generation-Toolkit"><a href="#Image-Generation-Toolkit" class="headerlink" title="Image Generation Toolkit"></a>Image Generation Toolkit</h2><p>Generative Adversarial Networks(GAN): have discriminator to notice fake or real images.<br>Auto-regressive(AR):识别追踪算法<br>Non-AR Transformer<br>Difussion: add noise and then denoising.  </p>
<h2 id="Diffusion-models"><a href="#Diffusion-models" class="headerlink" title="Diffusion models"></a>Diffusion models</h2><p>Intuition: generate images in one path, step-by-step manner. Add noise to the image, until it is randomly noisy, then based on the labels, do the reverse sampling process to denoise the noisy images.  </p>
<h2 id="Text-to-Image-Generation"><a href="#Text-to-Image-Generation" class="headerlink" title="Text to Image Generation"></a>Text to Image Generation</h2><p><strong>DALL-E</strong>.  </p>
<p>Based on CLIP.<br>A text prompt is imput into a text encoder that is trained to map the prompt to a representation space.  </p>
<p>它主要包括三个部分：CLIP，先验模块prior和img decoder。其中CLIP又包含text encoder和img encoder。  </p>
<p><strong>Imagen</strong>.  </p>
<p>Simpler than DALL-E.  </p>
<p>Frozen language model providing text embeddings to all diffusion models.  </p>
<h2 id="Text-to-Visual"><a href="#Text-to-Visual" class="headerlink" title="Text to Visual"></a>Text to Visual</h2><p>text to video</p>
<p>text to 3D shapes(家装)</p>
<p>text to motions/navigations(动漫，游戏)</p>
<h2 id="Multi-modal-GPT4"><a href="#Multi-modal-GPT4" class="headerlink" title="Multi-modal GPT4"></a>Multi-modal GPT4</h2><p>strong zero-shot visual understanding and reasoning capability.  </p>
<h2 id="Image-to-Text-Generative-Models"><a href="#Image-to-Text-Generative-Models" class="headerlink" title="Image-to-Text Generative Models"></a>Image-to-Text Generative Models</h2><p>Model architectures:</p>
<pre><code>- Pre-trained image encoders
- Pre-trained language models
- Modules(to be trained) to connect the two modalities.
</code></pre><p><strong>LLaVA</strong> model.  </p>
<p>LLaVA architecture:<br>Provide language instructions. Turn these instructions into embeddings. At the same time, mapping images into embeddings that is understandable by the language model we will use. Under the help of CLIP image encoder and a projection matrix which will map the embedded image to the same dimension as the LLM input(linear projection).  </p>
<p>The LM is fixed, pre-trained, the CLIP encoder is only trained with CLIP dataset, then it is still useful to train on it instead of fixing it.  </p>
<p>LLaVA Training:<br>Step 1: Feature alignment — aligning the representation of Vision Encoder and LLM. </p>
<pre><code>- Both Vision Encoder and LLM are kept frozen.  
- The Only training parameter is W the projection matrix.  
</code></pre><p>Step 2: End to end fine-tuning  </p>
<pre><code>- Vision Encoder is kept frozen. The trainig params are W and LLM.  
</code></pre><p>Can also do Multimodal RLHF. </p>
<p>(End of lecture 21)</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://xiyahc.github.io">Xinyu(Xiyah) Chang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://xiyahc.github.io/2024/05/23/NLP-Connect-LMs-to-the-world/">https://xiyahc.github.io/2024/05/23/NLP-Connect-LMs-to-the-world/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/NLP/">NLP</a></div><div class="post_share"><div class="social-share" data-image="https://images.naturecraft.world/images/2021/06/17/wl-op-24se.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/05/23/JHUNLPNotes/"><img class="prev-cover" src="https://images.naturecraft.world/images/2021/06/17/wl-op-24se.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">JHU NLP CS 601-671 Notes Collection</div></div></a></div><div class="next-post pull-right"><a href="/2024/02/06/hello-world/"><img class="next-cover" src="https://images.naturecraft.world/images/2021/06/17/wl-op-24se.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Hello World</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2024/05/23/JHUNLPNotes/" title="JHU NLP CS 601-671 Notes Collection"><img class="cover" src="https://images.naturecraft.world/images/2021/06/17/wl-op-24se.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-23</div><div class="title">JHU NLP CS 601-671 Notes Collection</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://images.naturecraft.world/images/2024/02/23/selfie.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Xinyu(Xiyah) Chang</div><div class="author-info__description">Love & Adapt.</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">11</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">9</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/XiyahC"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/XiyahC" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://www.linkedin.com/in/xinyuc" target="_blank" title="LinkedIn"><i class="fa-brands fa-linkedin"></i></a><a class="social-icon" href="mailto:xiyahchang23@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content"></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-connect-vision-language"><span class="toc-number">1.</span> <span class="toc-text">1.connect vision - language</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#History"><span class="toc-number">1.1.</span> <span class="toc-text">History</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#How-to-encode-images"><span class="toc-number">1.2.</span> <span class="toc-text">How to encode images?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#How-to-encode-paired-image-text"><span class="toc-number">1.3.</span> <span class="toc-text">How to encode paired image-text?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Contrastive-Language-Image-Pre-training-CLIP"><span class="toc-number">1.4.</span> <span class="toc-text">Contrastive Language-Image Pre-training (CLIP)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#What-happened-after-CLIP"><span class="toc-number">1.5.</span> <span class="toc-text">What happened after CLIP?</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-generative-vision-language-model"><span class="toc-number">2.</span> <span class="toc-text">2.generative vision-language model</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Image-Generation-Toolkit"><span class="toc-number">2.1.</span> <span class="toc-text">Image Generation Toolkit</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Diffusion-models"><span class="toc-number">2.2.</span> <span class="toc-text">Diffusion models</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Text-to-Image-Generation"><span class="toc-number">2.3.</span> <span class="toc-text">Text to Image Generation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Text-to-Visual"><span class="toc-number">2.4.</span> <span class="toc-text">Text to Visual</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Multi-modal-GPT4"><span class="toc-number">2.5.</span> <span class="toc-text">Multi-modal GPT4</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Image-to-Text-Generative-Models"><span class="toc-number">2.6.</span> <span class="toc-text">Image-to-Text Generative Models</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/05/28/Determine-Number-of-Clusters-in-K-means/" title="Number of Clusters in K-means"><img src="https://images.naturecraft.world/images/2021/06/17/wl-op-24se.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Number of Clusters in K-means"/></a><div class="content"><a class="title" href="/2024/05/28/Determine-Number-of-Clusters-in-K-means/" title="Number of Clusters in K-means">Number of Clusters in K-means</a><time datetime="2024-05-28T04:00:00.000Z" title="Created 2024-05-28 00:00:00">2024-05-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/05/27/How-to-Ace-Data-Science-Interview-Notes/" title="Book Notes"><img src="https://images.naturecraft.world/images/2021/06/17/wl-op-24se.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Book Notes"/></a><div class="content"><a class="title" href="/2024/05/27/How-to-Ace-Data-Science-Interview-Notes/" title="Book Notes">Book Notes</a><time datetime="2024-05-27T04:00:00.000Z" title="Created 2024-05-27 00:00:00">2024-05-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/05/27/Overfitting-in-Predictive-Models/" title="Overfitting in Predictive Models"><img src="https://images.naturecraft.world/images/2021/06/17/wl-op-24se.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Overfitting in Predictive Models"/></a><div class="content"><a class="title" href="/2024/05/27/Overfitting-in-Predictive-Models/" title="Overfitting in Predictive Models">Overfitting in Predictive Models</a><time datetime="2024-05-27T04:00:00.000Z" title="Created 2024-05-27 00:00:00">2024-05-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/05/23/JHUNLPNotes/" title="JHU NLP CS 601-671 Notes Collection"><img src="https://images.naturecraft.world/images/2021/06/17/wl-op-24se.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="JHU NLP CS 601-671 Notes Collection"/></a><div class="content"><a class="title" href="/2024/05/23/JHUNLPNotes/" title="JHU NLP CS 601-671 Notes Collection">JHU NLP CS 601-671 Notes Collection</a><time datetime="2024-05-23T04:00:00.000Z" title="Created 2024-05-23 00:00:00">2024-05-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/05/23/NLP-Connect-LMs-to-the-world/" title="NLP-Connecting Language to the World"><img src="https://images.naturecraft.world/images/2021/06/17/wl-op-24se.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="NLP-Connecting Language to the World"/></a><div class="content"><a class="title" href="/2024/05/23/NLP-Connect-LMs-to-the-world/" title="NLP-Connecting Language to the World">NLP-Connecting Language to the World</a><time datetime="2024-05-23T04:00:00.000Z" title="Created 2024-05-23 00:00:00">2024-05-23</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2024 By Xinyu(Xiyah) Chang</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Everyday is a good day.</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>